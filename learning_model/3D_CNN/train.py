# -*- coding: utf-8 -*-
"""Terminal

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GmE3LEx2P3ChH3lcISa5qGOr1fOTQR9l

Accident Risk classificaiton
"""

import numpy as np
import os, fnmatch
import matplotlib.pyplot as plt
from keras import *

# General configurations
dataset_path = '/media/tnnguyen/7E3B52AF2CE273C0/Thesis/Final-Thesis-Output/raster_imgs/CRSA/dataset/3h_1h30mins'
WD = {
    'input': {
        'train' : {
          'factors'    : dataset_path + '/train_2014/in_seq/',
          'predicted'  : dataset_path + '/train_2014/out_seq/'    
        },
        'test' : {
          'factors'    : dataset_path + '/test_2015/in_seq/',
          'predicted'  : dataset_path + '/test_2015/out_seq/'
        }
        
    },    
    'output': {
        'model_weights' : './training_output/model/',
        'plots'         : './training_output/monitor/'
    }
}

FACTOR = {
    # factor channel index
    'Input_congestion'        : 0,
    'Input_rainfall'          : 1,
    #'Input_sns'               : 2,        
    'Input_accident'          : 3,
    'default'                 : 0
}

MAX_FACTOR = {
    'Input_congestion'        : 5000,#8514,
    'Input_rainfall'          : 150,#174,
    #'Input_sns'               : 7,        
    'Input_accident'          : 1,
    'default'                 : 5000,#8514
}

# Load training data
def loadDataFile(path):
    try:
        data = np.load(path)
        data = data['arr_0']
    except Exception:
        data = None

    return data

def appendFactorData(factorName, factorData, X):
    # Load data
    data = factorData[:, :, :, FACTOR[factorName]]
    data = np.expand_dims(data, axis=3)
    data = np.expand_dims(data, axis=0)
    
    if factorName == 'Input_accident':
        data[data > 0] = 1
    
    # Standardize data
    data = data.astype(float)
    data /= MAX_FACTOR[factorName]

    if X[factorName] is None:
        X[factorName] = data
    else:
        X[factorName] = np.vstack((X[factorName], data))

    return X

def createBatch(batchSize, dataFiles, trainRatio=.8, mode='train'):
    # Initalize data
    X = {}
    for key in FACTOR.keys():
        X[key] = None
    
    y = {}
    y['default'] = None    
    
    numDataFiles = len(dataFiles)
    i = 0
    while i < batchSize:
        fileId = np.random.randint(low=0, high=int(numDataFiles), size=1)
        fileId = fileId[0]

        try:
            seqName = dataFiles[fileId]
            
            factorData = loadDataFile(WD['input'][mode]['factors'] + seqName)
            predictedData = loadDataFile(WD['input'][mode]['predicted'] + seqName)            
                
            if not (factorData is not None and predictedData is not None):
                continue

            # Load factors and predicted data
            for key in FACTOR.keys():
                X = appendFactorData(key, factorData, X)
            
            y = appendFactorData('default', predictedData, y)

        except Exception:
            continue
        
        i += 1

    del X['default']
    return X, y

# logging training progress
def logTrainingProgress(mode, contentLine):
    f = open(WD['output']['plots'] + 'loss_progress.csv', mode)
    f.write(contentLine)
    f.close()

# Evaluation
def mean_squared_error_eval(y_true, y_pred):
    return backend.eval(backend.mean(backend.square(y_pred - y_true)))

#100%/N * sum(abs(y_true-y_pred)) / y_true
def mean_absolute_percentage_error(y_true, y_pred):
    backend.set_epsilon(1.0)
    diff = backend.abs(y_true - y_pred) / \
           backend.clip(backend.abs(y_true), 1e-7, backend.epsilon())
      
    return 100. * backend.mean(diff)
  
def mean_absolute_percentage_error_eval(y_true, y_pred):
    backend.set_epsilon(1.0)    
    diff = backend.abs(y_true - y_pred) / \
           backend.clip(backend.abs(y_true), 1e-7, backend.epsilon())
      
    return backend.eval (100. * backend.mean(diff))

'''
def mean_absolute_percentage_error(y_true, y_pred):       
    return backend.mean( backend.abs(backend.sum(y_pred-y_true)) / backend.sum(y_true) ) * 100
  
def mean_absolute_percentage_error_eval(y_true, y_pred):       
    return backend.eval (  backend.mean( backend.abs(backend.sum(y_pred-y_true)) / backend.sum(y_true) ) * 100 )
'''

# Lower levels models
def buildCNN(cnnInputs, imgShape, filters, kernelSize, factorName, isFusion=False, cnnOutputs=None):
    if isFusion == True:
        cnnInput = layers.add(cnnOutputs, name='Fusion_{0}'.format(factorName))
    else:
        cnnInput = layers.Input(shape=imgShape, name='Input_{0}'.format(factorName))

    for i in range(len(filters)):
        counter = i+1
        if i == 0:
            cnnOutput = cnnInput

        cnnOutput = layers.Conv3D(filters=filters[i], kernel_size=kernelSize, strides=1, padding='same', activation='tanh',
                                  #kernel_initializer='he_normal',
                                  #kernel_regularizer=regularizers.l2(1e-5), activity_regularizer=regularizers.l2(1e-5),
                                  name='Conv3D_{0}{1}'.format(factorName, counter))(cnnOutput)
        cnnOutput = layers.BatchNormalization(name='BN_{0}{1}'.format(factorName, counter))(cnnOutput)
    
    if cnnInputs is not None:
        cnnModel = Model(inputs=cnnInputs, outputs=cnnOutput)
    else:
        cnnModel = Model(inputs=cnnInput, outputs=cnnOutput)
    return cnnModel
    
def buildPrediction(orgInputs, filters, kernelSize, lastOutputs=None):
    predictionOutput = None
    for i in range(len(filters)):
        counter = i + 1
        if i == 0:
            if lastOutputs is not None:
                predictionOutput = lastOutputs
            else:
                predictionOutput = orgInputs
                    
        if filters[i] != 0:
            predictionOutput = layers.Conv3D(filters=filters[i], kernel_size=kernelSize, strides=1, padding='same', activation='sigmoid', 
                                             #kernel_initializer='he_normal',
                                             name='Conv3D_prediction{0}1'.format(counter))(predictionOutput)        
            #predictionOutput = layers.LeakyReLU(alpha=0.1)(predictionOutput)
            #predictionOutput = layers.BatchNormalization(name='BN_prediction_{0}1'.format(counter))(predictionOutput)
        
        if filters[i] != 0:
            predictionOutput = layers.Conv3D(filters=filters[i], kernel_size=kernelSize, strides=1, padding='same', activation='relu', 
                                             #kernel_initializer='he_normal',
                                             #kernel_regularizer=regularizers.l2(1e-5), activity_regularizer=regularizers.l2(1e-5),
                                             name='Conv3D_prediction{0}2'.format(counter))(predictionOutput)
        #if filters[i] != 1:
        #    predictionOutput = layers.BatchNormalization(name='BN_prediction_{0}2'.format(counter))(predictionOutput)
        
    predictionOutput = layers.MaxPooling3D(pool_size=(2,1,1), name='output')(predictionOutput)

    predictionOutput = Model(inputs=orgInputs, outputs=predictionOutput)
    return predictionOutput

def buildCompleteModel(imgShape, filtersDict, kernelSizeDict):
    ########################################
    ## Define a CNN model for each factor ##
    ########################################
    filters = filtersDict['factors']
    kernelSize= kernelSizeDict['factors']

    filtersCongestion = list()
    for filter in range(len(filters)-1):
        filtersCongestion.append(int(filters[filter]*2.0))
    filtersCongestion.append(filters[-1])
    print(filtersCongestion)
    congestionCNNModel   = buildCNN(cnnInputs=None, imgShape=imgShape, filters=filtersCongestion, kernelSize=kernelSize, factorName='congestion')
    rainfallCNNModel     = buildCNN(cnnInputs=None, imgShape=imgShape, filters=filters, kernelSize=kernelSize, factorName='rainfall')
    #accidentCNNModel     = buildCNN(cnnInputs=None, imgShape=imgShape, filters=filters, kernelSize=kernelSize, factorName='accident')
    #snsCNNModel          = buildCNN(cnnInputs=None, imgShape=imgShape, filters=filters, kernelSize=kernelSize, factorName='sns')

    ##############################################
    ## Define a fused CNN model for all factors ##
    ##############################################
    filters = filtersDict['factors_fusion']
    kernelSize= kernelSizeDict['factors_fusion']

    fusedCNNModel       = buildCNN(cnnInputs=[congestionCNNModel.input, rainfallCNNModel.input],# accidentCNNModel.input],# snsCNNModel.input],
                                   cnnOutputs=[congestionCNNModel.output, rainfallCNNModel.output],# accidentCNNModel.output],# snsCNNModel.output],
                                   imgShape=imgShape,
                                   filters=filters, kernelSize=kernelSize,
                                   factorName='factors', isFusion=True
                                  )

    #################################
    ## Define the prediction model ##
    #################################
    filters = filtersDict['prediction']
    kernelSize= kernelSizeDict['prediction']
    predictionModel     = buildPrediction(orgInputs=[congestionCNNModel.input, rainfallCNNModel.input],# accidentCNNModel.input],# snsCNNModel.input],
                                          filters=filters,
                                          kernelSize=kernelSize,
                                          lastOutputs=fusedCNNModel.output
                                         )            

    return predictionModel

def calculateHA(data):
    data = data[0]
    predicted = np.average(data, axis=0)
    predicted = np.expand_dims(predicted, axis=0)
    #predicted = np.concatenate((predicted, predicted, predicted))
    predicted = np.concatenate((predicted, predicted))
    predicted = np.expand_dims(predicted, axis=0)

    return predicted

# Get the list of factors data files
print('Loading training data...')
trainDataFiles = fnmatch.filter(os.listdir(WD['input']['train']['factors']), '*30.npz')
trainDataFiles.sort()
numSamples = len(trainDataFiles)
print('Nunber of training data = {0}'.format(numSamples))

print('Loading testing data...')
testDataFiles = fnmatch.filter(os.listdir(WD['input']['test']['factors']), '*30.npz')

testDataFiles.sort()
numSamples = len(testDataFiles)
print('Nunber of testing data = {0}'.format(numSamples))

batchSize = 1
numEpochs = 1000000

###############################
## Define model architecture ##
###############################
imgShape = (6,20,250,1)
filtersDict = {}; filtersDict['factors'] = [128, 128, 128]; filtersDict['factors_fusion'] = [256, 256, 256, 128]; filtersDict['prediction'] = [64, 1]
#filtersDict = {}; filtersDict['factors'] = [32, 32, 32]; filtersDict['factors_fusion'] = [64, 64, 64, 32]; filtersDict['prediction'] = [16, 1]
kernelSizeDict = {}; kernelSizeDict['factors'] = (3,3,3); kernelSizeDict['factors_fusion'] = (3,3,3); kernelSizeDict['prediction'] = (3,3,3)

predictionModel = buildCompleteModel(imgShape, filtersDict, kernelSizeDict)
predictionModel.summary()
utils.plot_model(predictionModel,to_file='architecture.png',show_shapes=True)

from tensorflow.contrib.opt import LazyAdamOptimizer
#from tensorflow.python.keras.optimizers import TFOptimizer

tfopt = LazyAdamOptimizer()
#optimizer = TFOptimizer(tfopt)
lr = 5e-5
predictionModel.compile(optimizer=optimizers.Adam(lr=lr, decay=1e-5),
                        loss='mse',
                        metrics=['mse']
                       )
print('After configuring model...')
#predictionModel.load_weights('/home/tnnguyen/Thesis/Final-Thesis/prediction/training_output/epoch_5100_73_3.h5')

##############
## Training ##
##############
#predictionModel.load_weights(WD['output']['model_weights'] + 'epoch_3600.h5')
'''import tensorflow as tf
from keras.callbacks import TensorBoard
def write_log(callback, name, value, batch_no):    
    summary = tf.Summary()
    summary_value = summary.value.add()
    summary_value.simple_value = value
    summary_value.tag = name
    callback.writer.add_summary(summary, batch_no)
    callback.writer.flush()

log_path = './training_output/tensorboard/train'
callback_train = TensorBoard(log_path)
callback_train.set_model(predictionModel)
train_name = 'train_loss'

log_path = './training_output/tensorboard/test'
callback_test = TensorBoard(log_path)
callback_test.set_model(predictionModel)
test_name = 'test_loss'

tf.summary.merge_all()'''
from logger import Logger
train_logger = Logger('./training_output/tensorboard/train')
test_logger = Logger('./training_output/tensorboard/test')

trainLosses = list()
testLosses = list()
start = 1

if start <= 1:
    logTrainingProgress('w', 'epoch,training_loss,testing_loss,MSE,y_max, sum_y\n')

for epoch in range(start, numEpochs):
    # ============ Training progress ============#
    X, y = createBatch(batchSize, trainDataFiles)
    trainLoss = predictionModel.train_on_batch(X, y['default'])
    #write_log(callback_train, train_name, trainLoss[0], epoch)

    # test per epoch
    Xtest, ytest = createBatch(1, testDataFiles, mode='test')      
    ypredicted = predictionModel.predict(Xtest)
    
    testLoss = mean_squared_error_eval(ytest['default'], ypredicted)
    #write_log(callback_test, test_name, testLoss, epoch)

    # ============ TensorBoard logging ============#
    # Log the scalar values
    train_info = {
        'loss': trainLoss[0],
    }
    test_info = {
        'loss': testLoss,
    }

    for tag, value in train_info.items():
        train_logger.scalar_summary(tag, value, step=epoch)
    for tag, value in test_info.items():
        test_logger.scalar_summary(tag, value, step=epoch)

    #ypredicted *= MAX_FACTOR['Input_congestion']
    ytest = ytest['default']*MAX_FACTOR['Input_congestion']
    ypredicted = ypredicted*MAX_FACTOR['Input_congestion']
    MSE_normal = mean_squared_error_eval(ytest, ypredicted)
    
    result = '{0}\t{1}\t{2}\t{3}\t{4}\t{5}'.format(epoch, trainLoss, testLoss, int(MSE_normal), int(np.max(ypredicted)), int(np.sum(ypredicted)))
    print(result)
    
    trainLosses.append(trainLoss[0])
    testLosses.append(testLoss)
    
    
    # store model and record training progress
    if epoch % 100 == 0:
        #lr *= 0.95
        backend.set_value(predictionModel.optimizer.lr, lr)
        progress = '{0},{1},{2},{3}\n'.format(epoch, trainLoss, testLoss, int(MSE_normal), int(np.max(ypredicted)), int(np.sum(ypredicted)))
        logTrainingProgress('a', progress)
        
        # save model weight
        predictionModel.save_weights(WD['output']['model_weights'] \
                                     + 'epoch_' + str(epoch) + '.h5')
        
        # save plot
        plt.plot(trainLosses, color='red', label='Training loss')
        plt.plot(testLosses, color='blue', label='Testing loss')
        plt.xlabel('iterations')
        plt.ylabel('loss')
        plt.title('Mean square error')
        plt.legend()
        plt.savefig(WD['output']['plots'] + 'epoch_' + str(epoch) + '.png')
        plt.clf()

